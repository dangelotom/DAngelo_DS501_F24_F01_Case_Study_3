---
title: "DAngelo-DS501-F24-F01 - Case Study 3 EDA"
author: "Tom D'Angelo"
date: "2024-12-07"
output:
  pdf_document:
    latex_engine: xelatex
---

# Setup

```{r echo=FALSE, results="hide", warning=FALSE, message=FALSE}
# Libs
library("dplyr")
library("ggplot2")
library("corrplot")
library("readr")
library("vcd")
library("reshape2")
```

```{r}
# Read data
data <- read.csv("./data/CC_FRAUD.csv")
```

# Exploratory Data Analysis

-   **Dataset**

    -   *Title*: Online Credit Card Transactions

    -   *Source*: <https://www.kaggle.com/datasets/adityakadiwal/credit-card-fraudulent-transactions>

        -   Data was apparently scraped from web. See "Provenance" in Kaggle.

    -   *Primary Kaggle Contributor*: [Aditya Kadiwal](https://www.kaggle.com/adityakadiwal)

    -   *Brief Description*: Dataset contains \~90k credit card transactions with binary indicator variable identifying the transaction as "FRAUD" or "LEGIT". Data also contains transaction metadata including: masked domain name of customer's email, state and zip code of customer, time of transaction, amount of transaction, and anonymized data on the customer and transaction.

-   **Goals**

    -   Summary statistics

    -   Clean

    -   Distributions of continuous variables

    -   Bar plots of categorical variables

    -   Correlations and scatter plots (check for multicollinearity)

    -   Apply standard scaling to continuous variables, if needed.

    -   One-hot encode categorical variables.

-   **Project Goals**

    -   Since task is prediction of binary target variable (Fraud or Legit), logistic regression model could perform well. Therefore, overall project goal is to build logistic regression model.

-   See App Description for summary of EDA findings and model details.

### Summary Stats

```{r}
# Inspect
str(data)
```

```{r}
# Categorical and Continuous vars
categorical_vars <- c(
  "DOMAIN",
  "STATE",
  "ZIPCODE",
  "TIME1",
  "TIME2",
  "VIS1",
  "VIS2",
  "XRN1",
  "XRN2",
  "XRN3",
  "XRN4",
  "XRN5",
  "VAR1",
  "VAR2",
  "VAR5"
)

continuous_vars <- c(
  "VAR3",
  "VAR4",
  "TRN_AMT",
  "TOTAL_TRN_AMT"
)
```

```{r}
# Bar Plots - Cateogrical vars

# Iterate thru cat vars and plot
for (var in categorical_vars) {
  counts <- table(data[[var]])
  barplot(counts,
          main = paste("Bar Plot of", var),
          xlab = var,
          ylab = "Count",
          col = "blue",
          border = "black",
          las = 2)
}
```

-   Some categorical variables have large class imbalance. For example, the majority of values are 0 in variables VIS1, VIS2, and XRN4.

-   The distributions for the TIME variables look the same. Most transactions took place between the hours of about 10 and 17.

```{r}
# Histograms - Continuous vars

# Iterate thru cont vars and plot
for (var in continuous_vars) {
  hist(data[[var]],
       main = paste("Histogram of", var),
       xlab = var,
       col = "blue",
       border = "black")
}
```

-   TRN_AMT and TOTAL_TRN_AMT also appear to have identical distributions

### General Cleaning and Inspection

```{r}
# Missing vals
na_counts <- data.frame (
  col = names(data),
  na_count = colSums(is.na(data))
)

na_counts
```

-   No missing data

-   From above structure output, will need to encode TRN_TYPE (target var)

-   Might drop DOMAIN (masked customer email domain names), doesn't look like they'll add much predictive power.

-   TIME1 and TIME2 look identical. Will inspect further to confirm, drop one if superfluous.

-   VIS1:VAR2 and VAR5 look categorical. VIS1:XRN4 and VAR2 look binary, XRN5:VAR1 and VAR5 look nominal.

-   VAR3 and VAR4 look continuous. Although VAR3 values are floating point and VAR4 are integers. May convert to standard scale.

-   TRN_AMT and TOTAL_TRN_AMT appear identical. Will inspect further to confirm, drop one if superfluous.

#### Duplicate Rows

```{r}
# Check for duplicates
n_duplicated_rows <- sum(duplicated(data))
sprintf("There are %d duplicate rows.", n_duplicated_rows)
```

```{r}
# Drop duplicate rows
data <- data[!duplicated(data), ]
```

#### Encode Target Variable

```{r}
# Encode Transaction Type (TRN_TYPE)
# FRAUD == 1, LEGIT == 1
data$TRN_TYPE_ENCODED <- ifelse(data$TRN_TYPE == "FRAUD", 1, 0)

# Proportions
prop_trn_types <- prop.table(table(data$TRN_TYPE_ENCODED))
print("Proportions of Transaction Types:")
sprintf("Legit: %.2f", prop_trn_types["0"])
sprintf("Fraud: %.2f", prop_trn_types["1"])
```

-   Looks like there's severe class imbalance, most transactions are Legit (\~98%).

-   Will have to use evaluation metrics that are robust to large class imbalances:

    -   Area Under Receiver Operating Characteristic (AUROC), Area Under Precision-Recall Curve (AUPRC), and F1 Score

#### Inspect Similar Variables

```{r}
# Compare TIME1 and TIME2
print("TIME vars identical?")
print(identical(data$TIME1, data$TIME2))

print("AMT vars identical?")
print(identical(data$TRN_AMT, data$TOTAL_TRN_AMT))
```

-   Looks like the TIME vars are not identical, and neither are the AMT variables.

-   Will check further...

```{r}
# Inspect row differences for AMT vars
mismatch_rows <- data %>%
  filter(TRN_AMT != TOTAL_TRN_AMT)

print(mismatch_rows %>% select(DOMAIN, TRN_AMT, TOTAL_TRN_AMT))
```

```{r}
# Verify that in all cases where TOTAL != TRN_AMT TOTAL is 0
all(mismatch_rows$TOTAL_TRN_AMT == 0)
```

-   In all cases where TOTAL != TRN_AMT, TOTAL is 0 (45 rows). May be due to error.

    -   Drop TOTAL_TRN_AMT, keep TRN_AMT since all other rows will have same values.

```{r}
# Drop TOTAL_TRN_AMT
data <- data %>%
  select(-TOTAL_TRN_AMT)
```

```{r}
# Same checks for TIME vars
mismatch_rows <- data %>%
  filter(TIME1 != TIME2)

print(mismatch_rows %>% select(TIME1, TIME2))
```

-   Inspection of mismatches between TIME vars reveals more mismatch cases (1860) relative to the number of mismatches for AMT vars (45), even though both are negligible proportions of the whole dataset.

-   Mismatches for TIME vars are also not mismatched in same way as AMT vars (i.e., all mismatches in TOTAL var were 0).

-   Don't have quite enough information to say error or not, therefore keep both. May end up dropping or engineering into combined feature after multicollinearity analysis.

### Correlation

```{r}
# Plot correlations (Pearson method)

# Select columns for Pearson correlation testing
binary_vars <- c(
  "VIS1",
  "VIS2",
  "XRN1",
  "XRN2",
  "XRN3",
  "XRN4",
  "VAR2"
)

cont_vars = c(
  "VAR3",
  "VAR4",
  "TRN_AMT"
)

target_var <- c("TRN_TYPE_ENCODED")

pearson_data <- data[, c(binary_vars, cont_vars, target_var)]

# Correlation matrix
corr_matrix = cor(pearson_data, method="pearson")

# Plot
corrplot(corr_matrix, method="color", type="upper", tl.col="black", tl.srt=60)
```

-   Above heat map of Pearson correlations for continuous variables and binary variables reveal some strong associations:

    -   Looks like XRN2 and XRN3, and XRN1 and VAR4 are strongly correlated.

-   Interesting to note none of the variables appear to be strongly correlated with the target variable, TRN_TYPE_ENCODED. Indicates that a linear model like logistic regression may not be the best choice for this data. Could also indicate that none of these variables is a good predictor of transaction type...

```{r}
# Cramer's V for nominal variables

# Nominal vars
nom_vars <- c(
  "DOMAIN",
  "STATE",
  "ZIPCODE",
  "TIME1",
  "TIME2",
  "XRN5",
  "VAR1",
  "VAR5"
)

nom_data <- data[, c(nom_vars, target_var)]

# Cramer's V function
cramer_v_matrix <- function(data) {
  vars <- colnames(data)
  n <- length(vars)
  
  # Store results
  result <- matrix(NA, n, n, dimnames = list(vars, vars))
  
  for (i in 1:n) {
    for (j in 1:n) {
      if (i != j) {  # Skip diag (vars obviously associated w/ themselves)
        tbl <- table(data[[vars[i]]], data[[vars[j]]])
        result[i, j] <- assocstats(tbl)$cramer  # Cramer's V
      } else {
        result[i, j] <- NA
      }
    }
  }
  return(result)
}

# Calc associations
cramer_matrix <- cramer_v_matrix(nom_data)

# Melt cramer_matrix for plotting
cramer_long <- melt(cramer_matrix, na.rm = TRUE)

# Plot
ggplot(cramer_long, aes(Var1, Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(
    low = "blue", high = "red", mid = "white", 
    midpoint = 0.5, limit = c(0, 1), space = "Lab",
    name = "Cramer's V"
  ) +
  theme_minimal() +
  labs(title = "Cramer's V Heatmap: Nominal Variables", x = "Variable", y = "Variable") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

-   Above associations via Cramer's V indicate most nominal variables do not have strong associations with each other. Do see that ZIPCODE and STATE are strongly related (but not surprising). TIME variables are also strongly related. The TIME finding is supported by the earlier inspection of the bar plots of the TIME variables, which showed that the distributions were virtually identical. Does appear to be some slight association for STATE and DOMAIN variables (v \~ 0.50).

-   Inspect strong relationships...

```{r}
# Strong relationships
xrn_pear_corr <- data.frame(corr_matrix)["XRN2", "XRN3"]
var4_pear_corr <- data.frame(corr_matrix)["XRN1", "VAR4"]

sprintf("Pearson Correlation for XRN2 and XRN3: %.2f", xrn_pear_corr)
sprintf("Pearson Correlation for XRN1 and VAR4: %.2f", var4_pear_corr)
```

-   Above variable pairs are strongly correlated at 0.46 and -0.46. May consider combining into single feature or comparing models with/without one var from each pair to see if including/excluding impacts model performance. Alternatively, may give user control over which variables to include in model.

```{r}
# Inspect TIME var relationship - plot linear
plot(data$TIME1, data$TIME2, 
     main = "Scatter Plot of TIME1 vs TIME2",
     xlab = "TIME1",
     ylab = "TIME2",
     col = "black",
     pch = 16) 
```

-   From above plot, TIME variables have an almost perfect linear relationship, with some outliers.

```{r}
# Plot differences for TIME vars

# Cases where TIME1 and TIME2 are different
time_diffs_data <- data[data$TIME1 != data$TIME2, ]
sprintf("n cases where TIME1 != TIME2: %d", nrow(time_diffs_data))

# Calc differences
time_diffs_data$TIME_DIFF <- time_diffs_data$TIME1 - time_diffs_data$TIME2

# Plot differences
hist(time_diffs_data$TIME_DIFF,
     breaks = 20,
     col = "black",
     main = "Histogram of Differences (TIME1 - TIME2)",
     xlab = "Difference",
     border = "white")
```

-   There are relatively few cases where TIME1 != TIME2 (1860) - that is approximately 2% of all rows in data. Additionally, in the cases where the TIME vars are different, the difference is usually small.

-   As I think more about it, the two TIME vars may be the start and end hours of the transaction. I.e., if the transaction was started at 12:00PM and ended at 12:01PM, then the TIME1 hour would be 12 and the TIME2 hour would also be 12. In cases where transactions took a long time or occurred close to the start/end of each hour, the hour values for TIME1 and TIME2 would be different. I.e., transaction started at 12:58 and ended at 13:00, TIME1 would be 12 and TIME2 would be 13.

    -   Using this assumption, I'm not sure it makes sense to remove one of the TIME vars, since it could provide the model with useful information if, for example, fraudulent transactions usually take longer or occur closer to the top/bottom of the hour. It might make more sense, in that case to create a new variable of the differences between TIME1 and TIME2. May give the user that option in the UI.

-   TLDR - don't remove either TIME var for now.

### Encoding

-   Must be careful here. From above bar plots, some nominal variables have large number of unique categories (e.g., DOMAIN, STATE, ZIPCODE), while other variables have relatively few (e.g., VAR1 with 5 cats). To avoid curse of dimensionality, whereby the number of features in the dataset causes all observations to be very far from each other such that that model cannot identify any trends or patterns, I may have to group some categories in certain variables or use a different method of encoding.

    -   E.g., for STATE and ZIPCODE vars, might engineer a new feature that groups those vars together to create geographic "regions"

```{r}
# Counts of unique ZIPCODEs and STATEs
zip_counts <- data %>%
  count(ZIPCODE, name="Count") %>%
  arrange(desc(Count))

state_counts <- data %>%
  count(STATE, name="Count") %>%
  arrange(desc(Count))

zip_counts
state_counts
```

-   There are 94 unique ZIPCODE values, and 53 unique STATE values. Too many for vanilla one-hot encoding.

-   ZIPCODEs do seem to be grouped by first 1 or 2 digits.

```{r}
# Counts of ZIPCODE groups, group by first digit
data$ZIP_FirstDigit <- substr(data$ZIPCODE, 1, 1)

# Counts
zip_first_digit_counts <- data %>%
  count(ZIP_FirstDigit, name = "Count") %>%
  arrange(desc(Count))

zip_first_digit_counts
```

-   Grouping ZIPCODEs by first digit drastically reduces the number of categories.

-   May drop STATE and ZIP but keep first digit, but might lose variance...

```{r}
# Calculate variance of ZIPCODE, STATE, and ZIP_FirstDigit

# State
state_variance <- data %>%
  group_by(STATE) %>%
  summarize(Mean_Target = mean(TRN_TYPE_ENCODED, na.rm = TRUE)) %>%
  summarize(Variance_Explained = var(Mean_Target, na.rm = TRUE))

# ZIPCODE
zip_variance <- data %>%
  group_by(ZIPCODE) %>%
  summarize(Mean_Target = mean(TRN_TYPE_ENCODED, na.rm = TRUE)) %>%
  summarize(Variance_Explained = var(Mean_Target, na.rm = TRUE))

# ZIP_FirstDigit
zipFD_variance <- data %>%
  group_by(ZIP_FirstDigit) %>%
  summarize(Mean_Target = mean(TRN_TYPE_ENCODED, na.rm = TRUE)) %>%
  summarize(Variance_Explained = var(Mean_Target, na.rm = TRUE))

print("Target Variance Explained by Vars")
sprintf("STATE: %.2f", state_variance$Variance_Explained)
sprintf("ZIP: %.2f", zip_variance$Variance_Explained)
sprintf("ZIP_FirstDigit: %.2f", zipFD_variance$Variance_Explained)
```

-   Since none of these variables explains much of the target variance (supported by earlier Cramer's V statistics), I'll try grouping further to capture regional data...

```{r}
# Group STATE and ZIP_FirstDigit into regions
data <- data %>%
  mutate(State_ZipRegion = paste0(STATE, "-", ZIP_FirstDigit))

# Count unique "regions"
unique_regions <- n_distinct(data$State_ZipRegion)

print(unique_regions)
```

-   Result of state-first zip digit "regions" is same as number of states. Mapping zips and state onto regions gets us nowhere.

-   Encoding the nominal variables with high cardinality with the target (i.e., Target Encoding) would allow us to preserve these variables and reduce dimensionality.

```{r}
# Drop ZIP_FirstDigit and Region, not adding anything
data <- data %>%
  select(-c(ZIP_FirstDigit, State_ZipRegion))
```

# Final Thoughts

```{r}
# Cleaned df
data
```

-   After inspection and some cleaning, the above data is ready for the Application phase of this project.

    -   The target variable has been encoded to integers.

    -   Target encoding of the high-cardinality nominal variables (DOMAIN, STATE, and ZIPCODE) will take place in the Application. After the user defines the training size, the target encoding on these variables will take the group-wise target means from the training set and map those onto the testing set so as not to allow data leak at test time.

    -   Other low-cardinality categorical variables will be left as they are. Logistic regression models can handle categorical variables natively, and the user can interpret the model weights more easily.

    -   Continuous variables will not be standard-scaled, also making the weights directly interpretable.

```{r}
# Export to CSV
# write.csv(data, "CC_FRAUD_cleaned.csv", row.names=FALSE)
```
